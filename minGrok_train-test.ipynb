{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMw-8l3kUA5f"
   },
   "source": [
    "# Train and test your own minGrok (or load mine)\n",
    "\n",
    "### first the setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JOHHIHcjeWzN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./venv/lib/python3.10/site-packages')\n",
    "import dataclasses\n",
    "from model import *\n",
    "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oTbAKuix5nt",
    "outputId": "6237f171-09c6-40ac-bf3d-e96a3bdb3928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnx2ACOizX3-",
    "outputId": "fa6c8020-9b55-4702-92b9-f1bd70d6f828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    # v was defined earlier when we loaded TinyShakespeare. In Grok it's 131,072\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "\n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256 # in Grok it's 8,192\n",
    "\n",
    "    # The number of blocks in the model.\n",
    "    num_layers: int = 4 # In Grok it's 64\n",
    "\n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4 # In Grok it's 48\n",
    "\n",
    "    # The number of key-value heads for implementing attention.\n",
    "    num_key_value_heads: int = 1 # In Grok it's 8\n",
    "\n",
    "    # The hidden size of the model, AKA the embedding dimension. Each token embedding vector will be this long\n",
    "    hidden_size: int = 96 # In Grok it's 6,144\n",
    "\n",
    "    # How much wider should the inner dimension of the experts be than the model's embedding dimension?\n",
    "    embedding_multiplier_scale: int = 2 # In Grok it's 8 \n",
    "\n",
    "    # how many experts?\n",
    "    tot_num_experts: int = 4 # in Grok it's 8\n",
    "\n",
    "    # how many active experts per token?\n",
    "    chosen_num_experts: int = 2 # in Grok it's also 2\n",
    "\n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 24 # In Grok it's 128\n",
    "\n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-5 # this is to promote numerical stability & prevent dividing by 0\n",
    "\n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0 # Grok and most models use 10,000\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too\n",
    "\n",
    "    # whether to use a linear layer after normalization\n",
    "    use_scale: bool = True # same in Grok\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # the dropout rate to use during training\n",
    "    dropout = 0.05\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "I5hAq2VcvpdS"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F6M2WCXs_Vld"
   },
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oaiM9_Od_Vnv"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to periodically estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pdPqc-L_VqU",
    "outputId": "fea4a729-a9d5-437c-90fb-663e70ab03f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n",
      "minGrok(\n",
      "  (embedder): Embedding(128, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
      "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (moe): MoELayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x Expert(\n",
      "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
      "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (router): Router(\n",
      "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_moe_norm): RMSNorm()\n",
      "      (post_moe_norm): RMSNorm()\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate a new model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9KhAPH5g_VsX"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 250\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OECt3NLpBGKc",
    "outputId": "59d3397d-4a49-41be-c2eb-f3804dad0533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 90.9870, val loss 90.8024, time elapsed: 1.27 seconds\n",
      "step 250: train loss 51.4524, val loss 51.7193, time elapsed: 258.02 seconds\n",
      "step 500: train loss 37.4007, val loss 37.8576, time elapsed: 498.15 seconds\n",
      "step 750: train loss 26.1541, val loss 26.4934, time elapsed: 734.67 seconds\n",
      "step 1000: train loss 17.6011, val loss 17.9100, time elapsed: 967.24 seconds\n",
      "step 1250: train loss 11.4452, val loss 11.7783, time elapsed: 1201.66 seconds\n",
      "step 1500: train loss 7.4405, val loss 7.5937, time elapsed: 1434.68 seconds\n",
      "step 1750: train loss 4.9769, val loss 5.0984, time elapsed: 1667.42 seconds\n",
      "step 2000: train loss 3.8413, val loss 3.9789, time elapsed: 1902.65 seconds\n",
      "step 2250: train loss 3.3733, val loss 3.5052, time elapsed: 2132.47 seconds\n",
      "step 2500: train loss 3.1502, val loss 3.2822, time elapsed: 2364.87 seconds\n",
      "step 2750: train loss 3.0159, val loss 3.1541, time elapsed: 2596.28 seconds\n",
      "step 3000: train loss 2.9060, val loss 3.0407, time elapsed: 2828.71 seconds\n",
      "step 3250: train loss 2.8191, val loss 2.9600, time elapsed: 3073.97 seconds\n",
      "step 3500: train loss 2.7372, val loss 2.9083, time elapsed: 3306.13 seconds\n",
      "step 3750: train loss 2.6833, val loss 2.8297, time elapsed: 3538.14 seconds\n",
      "step 4000: train loss 2.6256, val loss 2.7961, time elapsed: 3770.08 seconds\n",
      "step 4250: train loss 2.5721, val loss 2.7590, time elapsed: 4001.70 seconds\n",
      "step 4500: train loss 2.5077, val loss 2.7271, time elapsed: 4231.84 seconds\n",
      "step 4750: train loss 2.4655, val loss 2.7029, time elapsed: 4462.57 seconds\n",
      "step 4999: train loss 2.4262, val loss 2.6485, time elapsed: 4692.34 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-embedding_multiplier_scale{config.embedding_multiplier_scale}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{max_iters}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3z_2iYbvqUV"
   },
   "source": [
    "### Alternatively, you can load the 1m parameter model I already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "B2exYhJGvxDt",
    "outputId": "b23129f9-b5b8-42c6-fd20-9be5857b859f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGrok(\n",
       "  (embedder): Embedding(128, 96)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (mqa): MQA(\n",
       "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
       "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (moe): MoELayer(\n",
       "        (experts): ModuleList(\n",
       "          (0-3): 4 x Expert(\n",
       "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
       "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (router): Router(\n",
       "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pre_mqa_norm): RMSNorm()\n",
       "      (post_mqa_norm): RMSNorm()\n",
       "      (pre_moe_norm): RMSNorm()\n",
       "      (post_moe_norm): RMSNorm()\n",
       "      (drop): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20|22-43-59.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFa4Pfi2vx3e"
   },
   "source": [
    "### Testing (performing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK5bkaFmv1dH",
    "outputId": "34c281b7-8870-4292-8be9-abb90af6688c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou mut thououn:\n",
      "To aperviles.\n",
      "\n",
      "JULIO:\n",
      "Eitrn, chain shall bturly tee'd no true,\n",
      "Have his the prote not, sian and ever our\n",
      "wer,\n",
      "And come my love place so icht sirireffe:\n",
      "Ar,\n",
      "And out were God at himbly sleet's are fair; I have more a come these\n",
      "From srot is with 'ful lords reperld,\n",
      "Boy chide.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hikwp10DQQEb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
