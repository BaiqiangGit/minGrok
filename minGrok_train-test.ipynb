{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMw-8l3kUA5f"
   },
   "source": [
    "# Train and test your own minGrok (or load mine)\n",
    "\n",
    "### first the setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JOHHIHcjeWzN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./venv/lib/python3.10/site-packages')\n",
    "import dataclasses\n",
    "from model import *\n",
    "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oTbAKuix5nt",
    "outputId": "6237f171-09c6-40ac-bf3d-e96a3bdb3928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnx2ACOizX3-",
    "outputId": "fa6c8020-9b55-4702-92b9-f1bd70d6f828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    # v was defined earlier when we loaded TinyShakespeare. In Grok it's 131,072\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "\n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256 # in Grok it's 8,192\n",
    "\n",
    "    # The number of layers in the model.\n",
    "    num_layers: int = 4 # In Grok it's 64\n",
    "\n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4 # In Grok it's 48\n",
    "\n",
    "    # The number of key-value heads for implementing attention.\n",
    "    num_key_value_heads: int = 1 # In Grok it's 8\n",
    "\n",
    "    # The hidden size of the model, AKA the embedding dimension. Each token embedding vector will be this long\n",
    "    hidden_size: int = 96 # In Grok it's 6,144\n",
    "\n",
    "    # How much wider should the inner dimension of the experts be than the model's embedding dimension?\n",
    "    embedding_multiplier_scale: int = 2 # In Grok it's roughly 5.33\n",
    "\n",
    "    # how many experts?\n",
    "    tot_num_experts: int = 4 # in Grok it's 8\n",
    "\n",
    "    # how many active experts per token?\n",
    "    chosen_num_experts: int = 2 # in Grok it's also 2\n",
    "\n",
    "    # what amount of noise should be injected into the router during training?\n",
    "    noise_std = 0.1 # the value for Grok has not been shared\n",
    "\n",
    "    # When we create a loss to encourage all experts to be used, how should that loss be weighted?\n",
    "    lambadada = 10 # Grok's value has not been shared\n",
    "    # excuse my silly naming\n",
    "\n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 24 # In Grok it's 128\n",
    "\n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-5 # this is to promote numerical stability & prevent dividing by 0\n",
    "\n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0 # Grok and most models use 10,000\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too\n",
    "\n",
    "    # whether to use a linear layer after normalization\n",
    "    use_scale: bool = True # same in Grok\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # the dropout rate to use during training\n",
    "    dropout = 0.05\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I5hAq2VcvpdS"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F6M2WCXs_Vld"
   },
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oaiM9_Od_Vnv"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to periodically estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pdPqc-L_VqU",
    "outputId": "fea4a729-a9d5-437c-90fb-663e70ab03f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n",
      "minGrok(\n",
      "  (embedder): Embedding(128, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
      "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (moe): MoELayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x Expert(\n",
      "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
      "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (router): Router(\n",
      "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_moe_norm): RMSNorm()\n",
      "      (post_moe_norm): RMSNorm()\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate a new model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9KhAPH5g_VsX"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 1e-5 # used 3e-4 for the first 5000 iters\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 2000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 100\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OECt3NLpBGKc",
    "outputId": "59d3397d-4a49-41be-c2eb-f3804dad0533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4603, val loss 2.6593, time elapsed: 1.15 seconds\n",
      "step 100: train loss 2.4402, val loss 2.6460, time elapsed: 102.51 seconds\n",
      "step 200: train loss 2.4155, val loss 2.6502, time elapsed: 201.02 seconds\n",
      "step 300: train loss 2.4238, val loss 2.6558, time elapsed: 302.31 seconds\n",
      "step 400: train loss 2.4197, val loss 2.6455, time elapsed: 399.47 seconds\n",
      "step 500: train loss 2.4427, val loss 2.6603, time elapsed: 496.25 seconds\n",
      "step 600: train loss 2.4317, val loss 2.6517, time elapsed: 594.00 seconds\n",
      "step 700: train loss 2.4523, val loss 2.6506, time elapsed: 695.84 seconds\n",
      "step 800: train loss 2.4124, val loss 2.6615, time elapsed: 799.43 seconds\n",
      "step 900: train loss 2.4369, val loss 2.6301, time elapsed: 900.91 seconds\n",
      "step 1000: train loss 2.4354, val loss 2.6568, time elapsed: 1002.66 seconds\n",
      "step 1100: train loss 2.4033, val loss 2.6486, time elapsed: 1098.36 seconds\n",
      "step 1200: train loss 2.4403, val loss 2.6508, time elapsed: 1194.98 seconds\n",
      "step 1300: train loss 2.4188, val loss 2.6256, time elapsed: 1291.53 seconds\n",
      "step 1400: train loss 2.4036, val loss 2.6346, time elapsed: 1388.99 seconds\n",
      "step 1500: train loss 2.4113, val loss 2.6279, time elapsed: 1485.93 seconds\n",
      "step 1600: train loss 2.4166, val loss 2.6373, time elapsed: 1582.42 seconds\n",
      "step 1700: train loss 2.4311, val loss 2.6474, time elapsed: 1678.93 seconds\n",
      "step 1800: train loss 2.4233, val loss 2.6146, time elapsed: 1775.62 seconds\n",
      "step 1900: train loss 2.4098, val loss 2.6181, time elapsed: 1872.80 seconds\n",
      "step 1999: train loss 2.4265, val loss 2.6327, time elapsed: 1978.33 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-embedding_multiplier_scale{config.embedding_multiplier_scale}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "            f'-tot_num_experts{config.tot_num_experts}'\n",
    "            f'-chosen_num_experts{config.chosen_num_experts}'\n",
    "            f'-use_scale{config.use_scale}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{max_iters}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d_%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3z_2iYbvqUV"
   },
   "source": [
    "### Alternatively, you can load the 1m parameter model I already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "B2exYhJGvxDt",
    "outputId": "b23129f9-b5b8-42c6-fd20-9be5857b859f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGrok(\n",
       "  (embedder): Embedding(128, 96)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (mqa): MQA(\n",
       "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
       "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (moe): MoELayer(\n",
       "        (experts): ModuleList(\n",
       "          (0-3): 4 x Expert(\n",
       "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
       "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (router): Router(\n",
       "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pre_mqa_norm): RMSNorm()\n",
       "      (post_mqa_norm): RMSNorm()\n",
       "      (pre_moe_norm): RMSNorm()\n",
       "      (post_moe_norm): RMSNorm()\n",
       "      (drop): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-tot_num_experts4-chosen_num_experts2-use_scaleTrue-batch32-train_iter5000--2024-03-21_18-20-32.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFa4Pfi2vx3e"
   },
   "source": [
    "### Testing (performing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK5bkaFmv1dH",
    "outputId": "34c281b7-8870-4292-8be9-abb90af6688c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou in.\n",
      "\n",
      "Tome?\n",
      "\n",
      "Nurse:\n",
      "Third peaguisrener:\n",
      "Lo, show and go yours, here mace meraticome\n",
      "For a thee be oneeget and the lambron a it-ntard; whileTHerle you fair murfeen a 'tis to like.\n",
      "\n",
      "MENEnguill Yort death their honour mind,\n",
      "If such therese the curry woront, I that mine,\n",
      "Why the stays is of him in still.\n",
      "\n",
      "F\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hikwp10DQQEb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
