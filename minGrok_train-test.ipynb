{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMw-8l3kUA5f"
   },
   "source": [
    "# Train and test your own minGrok (or load mine)\n",
    "\n",
    "### first the setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JOHHIHcjeWzN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./venv/lib/python3.10/site-packages')\n",
    "import dataclasses\n",
    "from model import *\n",
    "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oTbAKuix5nt",
    "outputId": "6237f171-09c6-40ac-bf3d-e96a3bdb3928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnx2ACOizX3-",
    "outputId": "fa6c8020-9b55-4702-92b9-f1bd70d6f828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    # v was defined earlier when we loaded TinyShakespeare. In Grok it's 131,072\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "\n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256 # in Grok it's 8,192\n",
    "\n",
    "    # The number of layers in the model.\n",
    "    num_layers: int = 4 # In Grok it's 64\n",
    "\n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4 # In Grok it's 48\n",
    "\n",
    "    # The number of key-value heads for implementing attention.\n",
    "    num_key_value_heads: int = 1 # In Grok it's 8\n",
    "\n",
    "    # The hidden size of the model, AKA the embedding dimension. Each token embedding vector will be this long\n",
    "    hidden_size: int = 96 # In Grok it's 6,144\n",
    "\n",
    "    # How much wider should the inner dimension of the experts be than the model's embedding dimension?\n",
    "    embedding_multiplier_scale: int = 2 # In Grok it's roughly 5.33\n",
    "\n",
    "    # how many experts?\n",
    "    tot_num_experts: int = 4 # in Grok it's 8\n",
    "\n",
    "    # how many active experts per token?\n",
    "    chosen_num_experts: int = 2 # in Grok it's also 2\n",
    "\n",
    "    # what amount of noise should be injected into the router during training?\n",
    "    noise_std = 0.1 # the value for Grok has not been shared\n",
    "\n",
    "    # When we create a loss to encourage all experts to be used, how should that loss be weighted?\n",
    "    lambadada = 10 # Grok's value has not been shared\n",
    "    # excuse my silly naming\n",
    "\n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 24 # In Grok it's 128\n",
    "\n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-5 # this is to promote numerical stability & prevent dividing by 0\n",
    "\n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0 # Grok and most models use 10,000\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too\n",
    "\n",
    "    # whether to use a linear layer after normalization\n",
    "    use_scale: bool = True # same in Grok\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # the dropout rate to use during training\n",
    "    dropout = 0.05\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I5hAq2VcvpdS"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F6M2WCXs_Vld"
   },
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oaiM9_Od_Vnv"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to periodically estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pdPqc-L_VqU",
    "outputId": "fea4a729-a9d5-437c-90fb-663e70ab03f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n",
      "minGrok(\n",
      "  (embedder): Embedding(128, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
      "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (moe): MoELayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x Expert(\n",
      "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
      "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (router): Router(\n",
      "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_moe_norm): RMSNorm()\n",
      "      (post_moe_norm): RMSNorm()\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate a new model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9KhAPH5g_VsX"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 100\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OECt3NLpBGKc",
    "outputId": "59d3397d-4a49-41be-c2eb-f3804dad0533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 93.9025, val loss 94.2138, time elapsed: 1.29 seconds\n",
      "step 100: train loss 63.2053, val loss 63.5041, time elapsed: 112.27 seconds\n",
      "step 200: train loss 56.7111, val loss 57.1537, time elapsed: 219.52 seconds\n",
      "step 300: train loss 49.9559, val loss 50.3031, time elapsed: 325.18 seconds\n",
      "step 400: train loss 44.4499, val loss 44.8236, time elapsed: 424.00 seconds\n",
      "step 500: train loss 39.4286, val loss 40.0152, time elapsed: 521.80 seconds\n",
      "step 600: train loss 34.9200, val loss 35.3958, time elapsed: 618.57 seconds\n",
      "step 700: train loss 30.5205, val loss 30.8940, time elapsed: 715.21 seconds\n",
      "step 800: train loss 26.5441, val loss 26.8567, time elapsed: 813.00 seconds\n",
      "step 900: train loss 22.9365, val loss 23.2085, time elapsed: 910.64 seconds\n",
      "step 1000: train loss 19.6185, val loss 19.9545, time elapsed: 1008.74 seconds\n",
      "step 1100: train loss 16.8094, val loss 17.0502, time elapsed: 1106.71 seconds\n",
      "step 1200: train loss 14.2278, val loss 14.4340, time elapsed: 1204.12 seconds\n",
      "step 1300: train loss 11.9997, val loss 12.2131, time elapsed: 1300.15 seconds\n",
      "step 1400: train loss 10.0113, val loss 10.1909, time elapsed: 1398.82 seconds\n",
      "step 1500: train loss 8.4196, val loss 8.5991, time elapsed: 1497.51 seconds\n",
      "step 1600: train loss 7.0273, val loss 7.2354, time elapsed: 1597.19 seconds\n",
      "step 1700: train loss 5.9472, val loss 6.1355, time elapsed: 1705.72 seconds\n",
      "step 1800: train loss 5.0722, val loss 5.2346, time elapsed: 1803.01 seconds\n",
      "step 1900: train loss 4.4657, val loss 4.6298, time elapsed: 1900.60 seconds\n",
      "step 2000: train loss 4.0658, val loss 4.1691, time elapsed: 1996.86 seconds\n",
      "step 2100: train loss 3.7509, val loss 3.8743, time elapsed: 2094.01 seconds\n",
      "step 2200: train loss 3.5440, val loss 3.6528, time elapsed: 2191.92 seconds\n",
      "step 2300: train loss 3.3936, val loss 3.5126, time elapsed: 2289.95 seconds\n",
      "step 2400: train loss 3.2961, val loss 3.4018, time elapsed: 2387.90 seconds\n",
      "step 2500: train loss 3.2127, val loss 3.3313, time elapsed: 2486.74 seconds\n",
      "step 2600: train loss 3.1377, val loss 3.2586, time elapsed: 2598.10 seconds\n",
      "step 2700: train loss 3.0731, val loss 3.1980, time elapsed: 2709.23 seconds\n",
      "step 2800: train loss 3.0244, val loss 3.1412, time elapsed: 2824.16 seconds\n",
      "step 2900: train loss 2.9677, val loss 3.0989, time elapsed: 2927.22 seconds\n",
      "step 3000: train loss 2.9288, val loss 3.0623, time elapsed: 3032.43 seconds\n",
      "step 3100: train loss 2.8867, val loss 3.0355, time elapsed: 3135.00 seconds\n",
      "step 3200: train loss 2.8561, val loss 2.9879, time elapsed: 3233.72 seconds\n",
      "step 3300: train loss 2.8354, val loss 2.9675, time elapsed: 3333.13 seconds\n",
      "step 3400: train loss 2.8114, val loss 2.9604, time elapsed: 3430.90 seconds\n",
      "step 3500: train loss 2.7731, val loss 2.9340, time elapsed: 3528.05 seconds\n",
      "step 3600: train loss 2.7413, val loss 2.8942, time elapsed: 3626.36 seconds\n",
      "step 3700: train loss 2.6997, val loss 2.8926, time elapsed: 3723.16 seconds\n",
      "step 3800: train loss 2.6971, val loss 2.8683, time elapsed: 3821.47 seconds\n",
      "step 3900: train loss 2.6830, val loss 2.8149, time elapsed: 3920.25 seconds\n",
      "step 4000: train loss 2.6444, val loss 2.8294, time elapsed: 4020.24 seconds\n",
      "step 4100: train loss 2.6397, val loss 2.8022, time elapsed: 4120.58 seconds\n",
      "step 4200: train loss 2.5895, val loss 2.7780, time elapsed: 4218.70 seconds\n",
      "step 4300: train loss 2.5796, val loss 2.7655, time elapsed: 4316.31 seconds\n",
      "step 4400: train loss 2.5314, val loss 2.7560, time elapsed: 4414.94 seconds\n",
      "step 4500: train loss 2.5477, val loss 2.7310, time elapsed: 4511.76 seconds\n",
      "step 4600: train loss 2.5180, val loss 2.7112, time elapsed: 4608.92 seconds\n",
      "step 4700: train loss 2.4831, val loss 2.7029, time elapsed: 4706.63 seconds\n",
      "step 4800: train loss 2.4805, val loss 2.7045, time elapsed: 4804.82 seconds\n",
      "step 4900: train loss 2.4671, val loss 2.6806, time elapsed: 4902.74 seconds\n",
      "step 4999: train loss 2.4414, val loss 2.6712, time elapsed: 5001.40 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-embedding_multiplier_scale{config.embedding_multiplier_scale}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "            f'-tot_num_experts{config.tot_num_experts}'\n",
    "            f'-chosen_num_experts{config.chosen_num_experts}'\n",
    "            f'-noise_std{config.noise_std}'\n",
    "            f'-lambadada{config.lambadada}'\n",
    "            f'-use_scale{config.use_scale}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{max_iters}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d_%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3z_2iYbvqUV"
   },
   "source": [
    "### Alternatively, you can load the 1m parameter model I already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "B2exYhJGvxDt",
    "outputId": "b23129f9-b5b8-42c6-fd20-9be5857b859f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGrok(\n",
       "  (embedder): Embedding(128, 96)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (mqa): MQA(\n",
       "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
       "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (moe): MoELayer(\n",
       "        (experts): ModuleList(\n",
       "          (0-3): 4 x Expert(\n",
       "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
       "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (router): Router(\n",
       "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pre_mqa_norm): RMSNorm()\n",
       "      (post_mqa_norm): RMSNorm()\n",
       "      (pre_moe_norm): RMSNorm()\n",
       "      (post_moe_norm): RMSNorm()\n",
       "      (drop): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-tot_num_experts4-chosen_num_experts2-noise_std0.1-lambadada10-use_scaleTrue-batch32-train_iter5000--2024-03-21_18-20-32.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFa4Pfi2vx3e"
   },
   "source": [
    "### Testing (performing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK5bkaFmv1dH",
    "outputId": "34c281b7-8870-4292-8be9-abb90af6688c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou the this pluck I mongatiaHe the from gost:\n",
      "It such the mpot sassengrant icain the mice,\n",
      "And near to it the pate'd men of the selee.\n",
      "Tholiio is secilo'd me, and you a with part\n",
      "Wardo wis?\n",
      "\n",
      "GLied is OKEENVO:\n",
      "Come stect'd shall dison theroroke be\n",
      "Buty! hap ne.\n",
      "\n",
      "ROMEO:\n",
      "Ret of ungon makn. u\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hikwp10DQQEb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
